{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22418a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.2.0-py3-none-any.whl (944 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m944.3/944.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.15.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting minigrid\n",
      "  Downloading minigrid-3.0.0-py3-none-any.whl (136 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from minigrid) (1.22.2)\n",
      "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from minigrid) (1.2.0)\n",
      "Collecting pygame>=2.4.0 (from minigrid)\n",
      "  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m235.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->minigrid) (0.0.4)\n",
      "Installing collected packages: pygame, minigrid\n",
      "Successfully installed minigrid-3.0.0 pygame-2.6.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Using device: cuda\n",
      "--- Curriculum Stage 1: 5x5 Maze ---\n",
      "Stage 1, Episode 100, Avg Reward (last 100): 0.32\n",
      "Stage 1, Episode 200, Avg Reward (last 100): 0.40\n",
      "Stage 1, Episode 300, Avg Reward (last 100): 0.54\n",
      "Stage 1, Episode 400, Avg Reward (last 100): 0.56\n",
      "Stage 1, Episode 500, Avg Reward (last 100): 0.36\n",
      "Stage 1, Episode 600, Avg Reward (last 100): 0.43\n",
      "Stage 1, Episode 700, Avg Reward (last 100): 0.03\n",
      "Stage 1, Episode 800, Avg Reward (last 100): 0.05\n",
      "Stage 1, Episode 900, Avg Reward (last 100): 0.05\n",
      "Stage 1, Episode 1000, Avg Reward (last 100): 0.01\n",
      "--- Stage 1 Complete ---\n",
      "--- Curriculum Stage 2: 8x8 Maze ---\n",
      "Stage 2, Episode 100, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 200, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 300, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 400, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 500, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 600, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 700, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 800, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 900, Avg Reward (last 100): 0.00\n",
      "Stage 2, Episode 1000, Avg Reward (last 100): 0.00\n",
      "--- Stage 2 Complete ---\n",
      "--- Curriculum Stage 3: 16x16 Maze ---\n",
      "Stage 3, Episode 100, Avg Reward (last 100): 0.00\n",
      "Stage 3, Episode 200, Avg Reward (last 100): 0.00\n",
      "Stage 3, Episode 300, Avg Reward (last 100): 0.01\n",
      "Stage 3, Episode 400, Avg Reward (last 100): 0.03\n",
      "Stage 3, Episode 500, Avg Reward (last 100): 0.13\n",
      "Stage 3, Episode 600, Avg Reward (last 100): 0.17\n",
      "Stage 3, Episode 700, Avg Reward (last 100): 0.05\n",
      "Stage 3, Episode 800, Avg Reward (last 100): 0.01\n",
      "Stage 3, Episode 900, Avg Reward (last 100): 0.07\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install minigrid\n",
    "!pip install matplotlib\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- ハイパーパラメータ設定 ---\n",
    "# カリキュラム学習の設定\n",
    "MAZE_SIZES = [5, 8, 16]\n",
    "EPISODES_PER_STAGE = 1000\n",
    "\n",
    "# DQNエージェントの設定\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 30000\n",
    "REPLAY_BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-5 \n",
    "TARGET_UPDATE_FREQ = 100\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 経験を保存するためのデータ構造 ---\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward', 'terminated'))\n",
    "\n",
    "# --- リプレイバッファ ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# --- CNNベースのQネットワーク ---\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_space_shape, action_space_n):\n",
    "        super(QNetwork, self).__init__()\n",
    "        h, w, c = obs_space_shape\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(c, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, c, h, w)\n",
    "            cnn_out_size = self.cnn(dummy_input).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cnn_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_space_n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device).float() / 255.0\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        cnn_out = self.cnn(x)\n",
    "        return self.fc(cnn_out)\n",
    "\n",
    "# --- DQNエージェント ---\n",
    "class DQNAgent:\n",
    "    def __init__(self, obs_space_shape, action_space_n):\n",
    "        self.action_space_n = action_space_n\n",
    "        self.policy_net = QNetwork(obs_space_shape, action_space_n).to(device)\n",
    "        self.target_net = QNetwork(obs_space_shape, action_space_n).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        eps_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
    "                        math.exp(-1. * self.steps_done / EPSILON_DECAY)\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.action_space_n)]], device=device, dtype=torch.long)\n",
    "\n",
    "    def update_model(self):\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "            next_q_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "            target_q_values = reward_batch + (GAMMA * next_q_values)\n",
    "\n",
    "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def sync_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# --- メインの学習ループ ---\n",
    "if __name__ == \"__main__\":\n",
    "    agent = None\n",
    "    all_stage_rewards = {}\n",
    "\n",
    "    for stage, size in enumerate(MAZE_SIZES):\n",
    "        print(f\"--- Curriculum Stage {stage + 1}: {size}x{size} Maze ---\")\n",
    "        env = gym.make(f'MiniGrid-Empty-{size}x{size}-v0')\n",
    "        env = ImgObsWrapper(env)\n",
    "        \n",
    "        if agent is None:\n",
    "            obs_shape = env.observation_space.shape\n",
    "            action_n = env.action_space.n\n",
    "            agent = DQNAgent(obs_shape, action_n)\n",
    "\n",
    "        stage_rewards = []\n",
    "        for episode in range(EPISODES_PER_STAGE):\n",
    "            obs, info = env.reset()\n",
    "            state = torch.tensor(obs, device=device).unsqueeze(0).float()\n",
    "            \n",
    "            terminated, truncated = False, False\n",
    "            episode_reward = 0\n",
    "            while not terminated and not truncated:\n",
    "                action = agent.select_action(state)\n",
    "                obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "                episode_reward += reward\n",
    "\n",
    "                next_state = torch.tensor(obs, device=device).unsqueeze(0).float() if not terminated else None\n",
    "                \n",
    "                agent.replay_buffer.push(state, action, next_state, \n",
    "                                         torch.tensor([reward], device=device), \n",
    "                                         torch.tensor(terminated, device=device))\n",
    "                state = next_state\n",
    "                agent.update_model()\n",
    "\n",
    "            stage_rewards.append(episode_reward)\n",
    "\n",
    "            if (episode + 1) % TARGET_UPDATE_FREQ == 0:\n",
    "                agent.sync_target_network()\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                avg_reward = np.mean(stage_rewards[-100:])\n",
    "                print(f\"Stage {stage+1}, Episode {episode+1}, Avg Reward (last 100): {avg_reward:.2f}\")\n",
    "        \n",
    "        all_stage_rewards[f\"{size}x{size}\"] = stage_rewards\n",
    "        print(f\"--- Stage {stage + 1} Complete ---\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # --- 結果のプロット ---\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    total_episodes = 0\n",
    "    for size, rewards in all_stage_rewards.items():\n",
    "        moving_avg = np.convolve(rewards, np.ones(100)/100, mode='valid')\n",
    "        episodes = np.arange(total_episodes, total_episodes + len(moving_avg))\n",
    "        plt.plot(episodes, moving_avg, label=f'Stage: {size}')\n",
    "        total_episodes += len(rewards)\n",
    "    \n",
    "    plt.title(\"DQN with Curriculum Learning Performance\")\n",
    "    plt.xlabel(\"Total Episodes\")\n",
    "    plt.ylabel(\"Average Reward (Moving Avg over 100 episodes)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"dqn_curriculum_performance.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b19fa71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
