{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBcVslBjL0NT",
        "outputId": "b2a11ac6-56f3-414f-82a1-8dde404cd6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minigrid\n",
            "  Downloading minigrid-3.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from minigrid) (1.2.0)\n",
            "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.6.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (0.0.4)\n",
            "Downloading minigrid-3.0.0-py3-none-any.whl (136 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minigrid\n",
            "Successfully installed minigrid-3.0.0\n",
            "Using device: cuda\n",
            "--- Baseline Experiment Started (Time Limit: 40 minutes) ---\n",
            "Episode 100, Time: 5.1 min, Avg Reward (last 100): 0.36\n",
            "Episode 200, Time: 12.0 min, Avg Reward (last 100): 0.17\n",
            "Episode 300, Time: 16.5 min, Avg Reward (last 100): 0.49\n",
            "Episode 400, Time: 24.5 min, Avg Reward (last 100): 0.04\n",
            "Episode 500, Time: 32.5 min, Avg Reward (last 100): 0.00\n",
            "Episode 600, Time: 38.4 min, Avg Reward (last 100): 0.31\n",
            "\n",
            "--- Training stopped due to time limit (40 minutes) ---\n",
            "Baseline results saved to baseline_rewards_40min.npy\n"
          ]
        }
      ],
      "source": [
        "!pip install minigrid\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import ImgObsWrapper\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import time\n",
        "\n",
        "# --- ハイパーパラメータ設定 ---\n",
        "TARGET_MAZE_SIZE = 16\n",
        "TRAINING_TIME_LIMIT_MINUTES = 40\n",
        "\n",
        "GAMMA = 0.99\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.05\n",
        "EPSILON_DECAY = 30000\n",
        "LEARNING_RATE = 3e-5\n",
        "REPLAY_BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 128\n",
        "TARGET_UPDATE_FREQ = 100\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'terminated'))\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "    def push(self, *args): self.memory.append(Transition(*args))\n",
        "    def sample(self, batch_size): return random.sample(self.memory, batch_size)\n",
        "    def __len__(self): return len(self.memory)\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, obs_space_shape, action_space_n):\n",
        "        super(QNetwork, self).__init__()\n",
        "        h, w, c = obs_space_shape\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(c, 16, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            cnn_out_size = self.cnn(torch.zeros(1, c, h, w)).shape[1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(cnn_out_size, 256), nn.ReLU(),\n",
        "            nn.Linear(256, action_space_n)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.to(device).float() / 255.0\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return self.fc(self.cnn(x))\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, obs_space_shape, action_space_n):\n",
        "        self.action_space_n = action_space_n\n",
        "        self.policy_net = QNetwork(obs_space_shape, action_space_n).to(device)\n",
        "        self.target_net = QNetwork(obs_space_shape, action_space_n).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
        "        self.replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        self.steps_done += 1\n",
        "        eps_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * math.exp(-1. * self.steps_done / EPSILON_DECAY)\n",
        "        if random.random() > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            return torch.tensor([[random.randrange(self.action_space_n)]], device=device, dtype=torch.long)\n",
        "\n",
        "    def update_model(self):\n",
        "        if len(self.replay_buffer) < BATCH_SIZE: return\n",
        "        transitions = self.replay_buffer.sample(BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "        with torch.no_grad():\n",
        "            next_q_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "            next_q_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
        "            target_q_values = reward_batch + (GAMMA * next_q_values)\n",
        "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def sync_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(f'MiniGrid-Empty-{TARGET_MAZE_SIZE}x{TARGET_MAZE_SIZE}-v0')\n",
        "    env = ImgObsWrapper(env)\n",
        "\n",
        "    agent = DQNAgent(env.observation_space.shape, env.action_space.n)\n",
        "\n",
        "    baseline_rewards = []\n",
        "    start_time = time.time()\n",
        "    episode_count = 0\n",
        "\n",
        "    print(f\"--- Baseline Experiment Started (Time Limit: {TRAINING_TIME_LIMIT_MINUTES} minutes) ---\")\n",
        "\n",
        "    while (time.time() - start_time) / 60 < TRAINING_TIME_LIMIT_MINUTES:\n",
        "        obs, info = env.reset()\n",
        "        state = torch.tensor(obs, device=device).unsqueeze(0).float()\n",
        "        terminated, truncated, episode_reward = False, False, 0\n",
        "\n",
        "        while not terminated and not truncated:\n",
        "            action = agent.select_action(state)\n",
        "            obs, reward, terminated, truncated, info = env.step(action.item())\n",
        "            episode_reward += reward\n",
        "            next_state = torch.tensor(obs, device=device).unsqueeze(0).float() if not terminated else None\n",
        "            agent.replay_buffer.push(state, action, next_state, torch.tensor([reward], device=device), torch.tensor(terminated, device=device))\n",
        "            state = next_state\n",
        "            agent.update_model()\n",
        "\n",
        "        baseline_rewards.append(episode_reward)\n",
        "        episode_count += 1\n",
        "\n",
        "        if episode_count % TARGET_UPDATE_FREQ == 0:\n",
        "            agent.sync_target_network()\n",
        "\n",
        "        if episode_count % 100 == 0:\n",
        "            avg_reward = np.mean(baseline_rewards[-100:])\n",
        "            elapsed_time = (time.time() - start_time) / 60\n",
        "            print(f\"Episode {episode_count}, Time: {elapsed_time:.1f} min, Avg Reward (last 100): {avg_reward:.2f}\")\n",
        "\n",
        "    print(f\"\\n--- Training stopped due to time limit ({TRAINING_TIME_LIMIT_MINUTES} minutes) ---\")\n",
        "    env.close()\n",
        "\n",
        "    np.save(\"baseline_rewards_40min.npy\", np.array(baseline_rewards))\n",
        "    print(\"Baseline results saved to baseline_rewards_40min.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Koichi09/Gunma_k.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yctihr5fQylt",
        "outputId": "16e03a9a-3c60-4638-b878-63baaf7f829d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Gunma_k'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 14 (delta 2), reused 5 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14/14), 8.53 KiB | 8.53 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nAiteRd5Spnd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}