{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-XshxINcKq",
        "outputId": "49c57b54-79b8-45cb-d05e-7e285df8c9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting minigrid\n",
            "  Downloading minigrid-3.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from minigrid) (1.2.0)\n",
            "Requirement already satisfied: pygame>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from minigrid) (2.6.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=0.28.1->minigrid) (0.0.4)\n",
            "Downloading minigrid-3.0.0-py3-none-any.whl (136 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/136.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.7/136.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minigrid\n",
            "Successfully installed minigrid-3.0.0\n",
            "Using device: cuda\n",
            "--- Curriculum Stage 1: 5x5 Maze ---\n",
            "Stage 1, Episode 100, Avg Reward (last 100): 0.31\n",
            "Stage 1, Episode 200, Avg Reward (last 100): 0.46\n",
            "Stage 1, Episode 300, Avg Reward (last 100): 0.55\n",
            "Stage 1, Episode 400, Avg Reward (last 100): 0.80\n",
            "Stage 1, Episode 500, Avg Reward (last 100): 0.86\n",
            "Stage 1, Episode 600, Avg Reward (last 100): 0.87\n",
            "Stage 1, Episode 700, Avg Reward (last 100): 0.84\n",
            "Stage 1, Episode 800, Avg Reward (last 100): 0.88\n",
            "Stage 1, Episode 900, Avg Reward (last 100): 0.90\n",
            "Stage 1, Episode 1000, Avg Reward (last 100): 0.91\n",
            "--- Stage 1 Complete ---\n",
            "--- Curriculum Stage 2: 8x8 Maze ---\n",
            "Stage 2, Episode 100, Avg Reward (last 100): 0.21\n",
            "Stage 2, Episode 200, Avg Reward (last 100): 0.47\n",
            "Stage 2, Episode 300, Avg Reward (last 100): 0.05\n",
            "Stage 2, Episode 400, Avg Reward (last 100): 0.11\n",
            "Stage 2, Episode 500, Avg Reward (last 100): 0.05\n",
            "Stage 2, Episode 600, Avg Reward (last 100): 0.02\n",
            "Stage 2, Episode 700, Avg Reward (last 100): 0.02\n",
            "Stage 2, Episode 800, Avg Reward (last 100): 0.07\n",
            "Stage 2, Episode 900, Avg Reward (last 100): 0.00\n",
            "Stage 2, Episode 1000, Avg Reward (last 100): 0.00\n",
            "--- Stage 2 Complete ---\n",
            "--- Curriculum Stage 3: 16x16 Maze ---\n",
            "Stage 3, Episode 100, Avg Reward (last 100): 0.07\n",
            "Stage 3, Episode 200, Avg Reward (last 100): 0.01\n",
            "Stage 3, Episode 300, Avg Reward (last 100): 0.01\n",
            "Stage 3, Episode 400, Avg Reward (last 100): 0.16\n",
            "Stage 3, Episode 500, Avg Reward (last 100): 0.02\n",
            "Stage 3, Episode 600, Avg Reward (last 100): 0.01\n",
            "Stage 3, Episode 700, Avg Reward (last 100): 0.01\n",
            "Stage 3, Episode 800, Avg Reward (last 100): 0.14\n",
            "Stage 3, Episode 900, Avg Reward (last 100): 0.00\n",
            "Stage 3, Episode 1000, Avg Reward (last 100): 0.00\n",
            "--- Stage 3 Complete ---\n",
            "\n",
            "Curriculum experiment results saved to curriculum_rewards.npy\n"
          ]
        }
      ],
      "source": [
        "!pip install minigrid\n",
        "import gymnasium as gym\n",
        "from minigrid.wrappers import ImgObsWrapper\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- ハイパーパラメータ設定 ---\n",
        "# カリキュラム学習の設定\n",
        "MAZE_SIZES = [5, 8, 16]\n",
        "EPISODES_PER_STAGE = 1000\n",
        "\n",
        "# DQNエージェントの設定\n",
        "GAMMA = 0.99\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.05\n",
        "EPSILON_DECAY = 30000\n",
        "LEARNING_RATE = 3e-5  # ベースラインと合わせた学習率\n",
        "REPLAY_BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 128\n",
        "TARGET_UPDATE_FREQ = 100\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 経験を保存するためのデータ構造\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'terminated'))\n",
        "\n",
        "# リプレイバッファ\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# CNNベースのQネットワーク\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, obs_space_shape, action_space_n):\n",
        "        super(QNetwork, self).__init__()\n",
        "        h, w, c = obs_space_shape\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(c, 16, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            cnn_out_size = self.cnn(torch.zeros(1, c, h, w)).shape[1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(cnn_out_size, 256), nn.ReLU(),\n",
        "            nn.Linear(256, action_space_n)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.to(device).float() / 255.0\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return self.fc(self.cnn(x))\n",
        "\n",
        "# DQNエージェント\n",
        "class DQNAgent:\n",
        "    def __init__(self, obs_space_shape, action_space_n):\n",
        "        self.action_space_n = action_space_n\n",
        "        self.policy_net = QNetwork(obs_space_shape, action_space_n).to(device)\n",
        "        self.target_net = QNetwork(obs_space_shape, action_space_n).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
        "        self.replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        self.steps_done += 1\n",
        "        eps_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
        "                        math.exp(-1. * self.steps_done / EPSILON_DECAY)\n",
        "        if random.random() > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            return torch.tensor([[random.randrange(self.action_space_n)]], device=device, dtype=torch.long)\n",
        "\n",
        "    def update_model(self):\n",
        "        if len(self.replay_buffer) < BATCH_SIZE: return\n",
        "        transitions = self.replay_buffer.sample(BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "        with torch.no_grad():\n",
        "            next_q_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "            next_q_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
        "            target_q_values = reward_batch + (GAMMA * next_q_values)\n",
        "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def sync_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "# --- メインの学習ループ ---\n",
        "if __name__ == \"__main__\":\n",
        "    agent = None\n",
        "    all_stage_rewards = {}\n",
        "\n",
        "    for stage, size in enumerate(MAZE_SIZES):\n",
        "        print(f\"--- Curriculum Stage {stage + 1}: {size}x{size} Maze ---\")\n",
        "        env = gym.make(f'MiniGrid-Empty-{size}x{size}-v0')\n",
        "        env = ImgObsWrapper(env)\n",
        "\n",
        "        if agent is None:\n",
        "            obs_shape = env.observation_space.shape\n",
        "            action_n = env.action_space.n\n",
        "            agent = DQNAgent(obs_shape, action_n)\n",
        "\n",
        "        stage_rewards = []\n",
        "        for episode in range(EPISODES_PER_STAGE):\n",
        "            obs, info = env.reset()\n",
        "            state = torch.tensor(obs, device=device).unsqueeze(0).float()\n",
        "            terminated, truncated, episode_reward = False, False, 0\n",
        "            while not terminated and not truncated:\n",
        "                action = agent.select_action(state)\n",
        "                obs, reward, terminated, truncated, info = env.step(action.item())\n",
        "                episode_reward += reward\n",
        "                next_state = torch.tensor(obs, device=device).unsqueeze(0).float() if not terminated else None\n",
        "                agent.replay_buffer.push(state, action, next_state, torch.tensor([reward], device=device), torch.tensor(terminated, device=device))\n",
        "                state = next_state\n",
        "                agent.update_model()\n",
        "\n",
        "            stage_rewards.append(episode_reward)\n",
        "\n",
        "            if (episode + 1) % TARGET_UPDATE_FREQ == 0:\n",
        "                agent.sync_target_network()\n",
        "\n",
        "            if (episode + 1) % 100 == 0:\n",
        "                avg_reward = np.mean(stage_rewards[-100:])\n",
        "                print(f\"Stage {stage+1}, Episode {episode+1}, Avg Reward (last 100): {avg_reward:.2f}\")\n",
        "\n",
        "        all_stage_rewards[f\"{size}x{size}\"] = stage_rewards\n",
        "        print(f\"--- Stage {stage + 1} Complete ---\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # 結果を保存\n",
        "    curriculum_rewards_flat = [reward for stage_rewards in all_stage_rewards.values() for reward in stage_rewards]\n",
        "    np.save(\"curriculum_rewards.npy\", np.array(curriculum_rewards_flat))\n",
        "    print(\"\\nCurriculum experiment results saved to curriculum_rewards.npy\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}